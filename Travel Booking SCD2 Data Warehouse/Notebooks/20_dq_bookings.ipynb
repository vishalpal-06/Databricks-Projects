{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34626858-b27a-4ebb-97db-345aa2e01be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281f954c-c5df-459a-ab84-0dd476dfad1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAVEL BOOKING SCD2 MERGE PROJECT - DATA QUALITY: BOOKING DATA VALIDATION\n",
    "# =============================================================================\n",
    "# Purpose: Native PySpark DQ validation for Booking data (Serverless Compatible)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "import datetime as _dt\n",
    "\n",
    "# =============================================================================\n",
    "# PARAMETER EXTRACTION\n",
    "# =============================================================================\n",
    "try:\n",
    "    arrival_date = dbutils.widgets.get(\"arrival_date\")\n",
    "except Exception:\n",
    "    arrival_date = _dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "catalog = \"travel_bookings\"\n",
    "dataset_name = \"booking_inc\"\n",
    "\n",
    "# =============================================================================\n",
    "# DQ RESULTS STORAGE SETUP\n",
    "# =============================================================================\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.ops\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.ops.dq_results (\n",
    "  business_date DATE,\n",
    "  dataset STRING,\n",
    "  check_name STRING,\n",
    "  status STRING,\n",
    "  constraint STRING,\n",
    "  message STRING,\n",
    "  recorded_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# SOURCE DATA PREPARATION\n",
    "# =============================================================================\n",
    "src = spark.table(f\"{catalog}.bronze.{dataset_name}\").where(F.col(\"business_date\") == F.to_date(F.lit(arrival_date)))\n",
    "\n",
    "# =============================================================================\n",
    "# DATA QUALITY CHECKS EXECUTION (NATIVE SQL)\n",
    "# =============================================================================\n",
    "# Perform single-pass aggregation for all metrics\n",
    "metrics = src.select(\n",
    "    F.count(\"*\").alias(\"row_count\"),\n",
    "    F.sum(F.when(F.col(\"customer_id\").isNull(), 1).otherwise(0)).alias(\"null_customer_id\"),\n",
    "    F.sum(F.when(F.col(\"amount\").isNull(), 1).otherwise(0)).alias(\"null_amount\"),\n",
    "    F.sum(F.when(F.col(\"amount\") < 0, 1).otherwise(0)).alias(\"neg_amount\"),\n",
    "    F.sum(F.when(F.col(\"quantity\") < 0, 1).otherwise(0)).alias(\"neg_quantity\"),\n",
    "    F.sum(F.when(F.col(\"discount\") < 0, 1).otherwise(0)).alias(\"neg_discount\")\n",
    ").collect()[0]\n",
    "\n",
    "# =============================================================================\n",
    "# DQ RESULTS TRANSFORMATION\n",
    "# =============================================================================\n",
    "# Map the collected metrics into the audit table schema\n",
    "results_data = [\n",
    "    (arrival_date, dataset_name, \"Size Check\", \"Success\" if metrics[\"row_count\"] > 0 else \"Error\", \n",
    "     \"hasSize > 0\", f\"Count: {metrics['row_count']}\"),\n",
    "    \n",
    "    (arrival_date, dataset_name, \"Completeness: customer_id\", \"Success\" if metrics[\"null_customer_id\"] == 0 else \"Error\", \n",
    "     \"isComplete\", f\"Nulls: {metrics['null_customer_id']}\"),\n",
    "    \n",
    "    (arrival_date, dataset_name, \"Completeness: amount\", \"Success\" if metrics[\"null_amount\"] == 0 else \"Error\", \n",
    "     \"isComplete\", f\"Nulls: {metrics['null_amount']}\"),\n",
    "    \n",
    "    (arrival_date, dataset_name, \"Integrity: amount\", \"Success\" if metrics[\"neg_amount\"] == 0 else \"Error\", \n",
    "     \"isNonNegative\", f\"Negatives: {metrics['neg_amount']}\"),\n",
    "    \n",
    "    (arrival_date, dataset_name, \"Integrity: quantity\", \"Success\" if metrics[\"neg_quantity\"] == 0 else \"Error\", \n",
    "     \"isNonNegative\", f\"Negatives: {metrics['neg_quantity']}\"),\n",
    "    \n",
    "    (arrival_date, dataset_name, \"Integrity: discount\", \"Success\" if metrics[\"neg_discount\"] == 0 else \"Error\", \n",
    "     \"isNonNegative\", f\"Negatives: {metrics['neg_discount']}\")\n",
    "]\n",
    "\n",
    "dq_results_df = (\n",
    "    spark.createDataFrame(\n",
    "        results_data,\n",
    "        [\"business_date\", \"dataset\", \"check_name\", \"status\", \"constraint\", \"message\"]\n",
    "    )\n",
    "    .withColumn(\"business_date\", F.to_date(F.col(\"business_date\")))\n",
    "    .withColumn(\"recorded_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# DQ RESULTS LOGGING & VALIDATION\n",
    "# =============================================================================\n",
    "display(dq_results_df)\n",
    "\n",
    "# Write to audit table\n",
    "dq_results_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.ops.dq_results\")\n",
    "\n",
    "# Fail if any check returned an 'Error' status\n",
    "if dq_results_df.filter(F.col(\"status\") == \"Error\").count() > 0:\n",
    "    raise ValueError(f\"DQ failed for {dataset_name}. Check {catalog}.ops.dq_results for details.\")\n",
    "\n",
    "print(f\"Booking DQ passed for {arrival_date}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "20_dq_bookings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
