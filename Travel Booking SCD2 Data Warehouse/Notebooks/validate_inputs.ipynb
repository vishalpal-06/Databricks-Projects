{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf05705-202b-4fe3-9d38-218d957c2a38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAVEL BOOKING SCD2 MERGE PROJECT - INPUT VALIDATION\n",
    "# =============================================================================\n",
    "# This notebook validates input parameters and file existence before processing\n",
    "# Purpose: Ensures all required inputs are available and creates audit logging\n",
    "# Dependencies: Requires booking and customer CSV files for the specified date\n",
    "# Output: Creates run_log table for pipeline tracking and validation\n",
    "\n",
    "import datetime as _dt\n",
    "\n",
    "# =============================================================================\n",
    "# PARAMETER EXTRACTION WITH DEFAULTS\n",
    "# =============================================================================\n",
    "# Extract widget parameters with fallback defaults for flexibility\n",
    "# arrival_date: Business date for processing (defaults to today)\n",
    "# catalog: Unity Catalog name (defaults to travel_bookings)\n",
    "# schema: Target schema (defaults to default)\n",
    "# base_volume: Base path for data files (defaults to /Volumes/{catalog}/{schema}/data)\n",
    "\n",
    "try:\n",
    "    arrival_date = dbutils.widgets.get(\"arrival_date\")\n",
    "except Exception:\n",
    "    arrival_date = _dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "try:\n",
    "    catalog = dbutils.widgets.get(\"catalog\")\n",
    "except Exception:\n",
    "    catalog = \"travel_bookings\"\n",
    "\n",
    "try:\n",
    "    schema = dbutils.widgets.get(\"schema\")\n",
    "except Exception:\n",
    "    schema = \"default\"\n",
    "    \n",
    "try:\n",
    "    base_volume = dbutils.widgets.get(\"base_volume\")\n",
    "except Exception:\n",
    "    base_volume = f\"/Volumes/{catalog}/{schema}/data\"\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, lit, to_date\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# =============================================================================\n",
    "# FILE PATH CONSTRUCTION\n",
    "# =============================================================================\n",
    "# Construct expected file paths based on business date\n",
    "# Format: /Volumes/{catalog}/{schema}/data/{data_type}_data/{file_type}_{date}.csv\n",
    "\n",
    "booking_path = f\"{base_volume}/booking_data/bookings_{arrival_date}.csv\"\n",
    "customer_path = f\"{base_volume}/customer_data/customers_{arrival_date}.csv\"\n",
    "\n",
    "# =============================================================================\n",
    "# FILE EXISTENCE VALIDATION\n",
    "# =============================================================================\n",
    "# Check if required input files exist before proceeding\n",
    "# Collects missing files and raises exception if any are not found\n",
    "\n",
    "missing = []\n",
    "try:\n",
    "    dbutils.fs.ls(booking_path)\n",
    "except Exception:\n",
    "    missing.append(booking_path)\n",
    "\n",
    "try:\n",
    "    dbutils.fs.ls(customer_path)\n",
    "except Exception:\n",
    "    missing.append(customer_path)\n",
    "\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing input files: {missing}\")\n",
    "\n",
    "# =============================================================================\n",
    "# AUDIT LOGGING SETUP\n",
    "# =============================================================================\n",
    "# Create operations schema and run_log table for pipeline tracking\n",
    "# Tracks run_id, arrival_date, stage, status, message, and timestamp\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.ops\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.ops.run_log (\n",
    "  run_id STRING,\n",
    "  arrival_date DATE,\n",
    "  stage STRING,\n",
    "  status STRING,\n",
    "  message STRING,\n",
    "  recorded_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# LOG VALIDATION COMPLETION\n",
    "# =============================================================================\n",
    "# Record successful validation in run_log for audit trail\n",
    "# Uses timestamp-based run_id for uniqueness\n",
    "\n",
    "run_id = f\"nb-validate-{arrival_date}-{int(time.time())}\"\n",
    "log_df = spark.createDataFrame([\n",
    "    (run_id, arrival_date, \"validate_inputs\", \"STARTED\", \"Inputs validated\")\n",
    "], [\"run_id\",\"arrival_date\",\"stage\",\"status\",\"message\"])\n",
    "log_df = log_df.withColumn(\"arrival_date\", F.to_date(\"arrival_date\")).withColumn(\"recorded_at\", current_timestamp())\n",
    "log_df.write.mode(\"append\").saveAsTable(f\"{catalog}.ops.run_log\")\n",
    "\n",
    "print(\"Validation successful:\", booking_path, customer_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "validate_inputs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
