{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f76322e6-a34c-428a-ac10-8a723a67d83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213b350b-e9ad-46af-b4da-b831fbc006a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import datetime as _dt\n",
    "\n",
    "# 1. PARAMETERS (Consistent with your existing SCD2 project)\n",
    "try:\n",
    "    arrival_date = dbutils.widgets.get(\"arrival_date\")\n",
    "except Exception:\n",
    "    arrival_date = _dt.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "catalog = \"travel_bookings\"\n",
    "schema = \"default\"\n",
    "dataset_name = \"customer_inc\"\n",
    "\n",
    "# 2. SOURCE DATA\n",
    "src = spark.table(f\"{catalog}.bronze.{dataset_name}\").where(F.col(\"business_date\") == F.to_date(F.lit(arrival_date)))\n",
    "\n",
    "# 3. DEFINE QUALITY CHECKS (Native Spark SQL)\n",
    "# Row Count Check\n",
    "row_count = src.count()\n",
    "has_size_passed = row_count > 0\n",
    "\n",
    "# Completeness Checks (Null counts for specific columns)\n",
    "# We aggregate the null counts in a single pass for efficiency\n",
    "null_counts = src.select(\n",
    "    F.sum(F.when(F.col(\"customer_name\").isNull(), 1).otherwise(0)).alias(\"null_name\"),\n",
    "    F.sum(F.when(F.col(\"customer_address\").isNull(), 1).otherwise(0)).alias(\"null_address\"),\n",
    "    F.sum(F.when(F.col(\"email\").isNull(), 1).otherwise(0)).alias(\"null_email\")\n",
    ").collect()[0]\n",
    "\n",
    "# 4. PREPARE DQ RESULTS FOR LOGGING\n",
    "# Define conditions for a \"Success\" status\n",
    "results_list = [\n",
    "    (arrival_date, dataset_name, \"Size Check\", \"Success\" if has_size_passed else \"Error\", \n",
    "     \"Size > 0\", f\"Found {row_count} rows\"),\n",
    "    \n",
    "    (arrival_date, dataset_name, \"Completeness: customer_name\", \"Success\" if null_counts[\"null_name\"] == 0 else \"Error\", \n",
    "     \"customer_name is not null\", f\"Found {null_counts['null_name']} null values\"),\n",
    "    \n",
    "    (arrival_date, dataset_name, \"Completeness: customer_address\", \"Success\" if null_counts[\"null_address\"] == 0 else \"Error\", \n",
    "     \"customer_address is not null\", f\"Found {null_counts['null_address']} null values\"),\n",
    "    \n",
    "    (arrival_date, dataset_name, \"Completeness: email\", \"Success\" if null_counts[\"null_email\"] == 0 else \"Error\", \n",
    "     \"email is not null\", f\"Found {null_counts['null_email']} null values\")\n",
    "]\n",
    "\n",
    "# Create DataFrame for logging\n",
    "dq_log_df = (\n",
    "    spark.createDataFrame(\n",
    "        results_list,\n",
    "        [\"business_date\", \"dataset\", \"check_name\", \"status\", \"constraint\", \"message\"]\n",
    "    )\n",
    "    .withColumn(\"business_date\", F.to_date(F.col(\"business_date\")))\n",
    "    .withColumn(\"recorded_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "dq_log_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.ops.dq_results\")\n",
    "\n",
    "# 5. STORAGE & ERROR HANDLING\n",
    "# Append to the ops table created in your previous step\n",
    "dq_log_df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{catalog}.ops.dq_results\")\n",
    "\n",
    "# Fail the pipeline if any check is 'Error'\n",
    "failed_checks = dq_log_df.filter(F.col(\"status\") == \"Error\")\n",
    "if failed_checks.count() > 0:\n",
    "    display(failed_checks)\n",
    "    raise ValueError(f\"Data Quality Failed for {dataset_name} on {arrival_date}. See {catalog}.ops.dq_results for details.\")\n",
    "\n",
    "print(f\"Customer DQ passed for {arrival_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a914f57-c56d-4073-81d5-76de6f665922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "21_dq_customers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
